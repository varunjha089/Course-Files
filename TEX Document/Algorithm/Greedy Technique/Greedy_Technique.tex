\documentclass[14pt]{extarticle}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{makecell}

\definecolor{codegray}{gray}{0.95}
\lstset{
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    columns=fullflexible,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{purple},
    morecomment=[l][\color{gray}]{\#}
}

\title{\textbf{Greedy Technique}}
\author{Varun Kumar}
\date{July 5, 2025}

\begin{document}

\maketitle

\section*{1. Logic}

Greedy algorithms build up a solution step by step by choosing the best available option at each stage. They don't backtrack or revise previous choices.

\begin{tcolorbox}[
  colback=white,
  colframe=black,
  title=Key Idea
]
At each step, choose the best local option with the hope that it leads to a globally optimal solution.
\end{tcolorbox}

\section*{2. When to Use}

\begin{itemize}
    \item The problem has the \textbf{Greedy Choice Property}.
    \item The problem exhibits \textbf{Optimal Substructure}.
    \item Fast approximation is acceptable.
\end{itemize}

\section*{3. General Pseudocode}

\begin{lstlisting}[language=Python]
# Generic structure of a greedy algorithm
def greedyAlgorithm(problem):
    solution = []
    while not problem.complete():
        candidate = select_best_candidate(problem)
        if is_feasible(candidate):
            solution.append(candidate)
    return solution
\end{lstlisting}

\newpage
\section*{4. Example 1: Activity Selection}

\subsection*{Problem}
Given $n$ activities with start and finish times, select the maximum number of non-overlapping activities.

\subsection*{Greedy Strategy}
Always pick the activity that finishes earliest.

\subsection*{Example}

Given:  
\texttt{[(1, 3), (2, 5), (4, 7), (1, 8), (5, 9), (8, 10), (9, 11), (11, 14), (13, 16)]}

\begin{itemize}
    \item Sort by finish time: \texttt{(1, 3), (2, 5), (4, 7), (1, 8), (5, 9), (8, 10), (9, 11), (11, 14), (13, 16)}
    \item Select (1, 3) → End time = 3
    \item Next valid = (4, 7) → End = 7
    \item Next = (8, 10) → End = 10
    \item Next = (11, 14) → End = 14
\end{itemize}

\textbf{Selected activities:} \texttt{[(1, 3), (4, 7), (8, 10), (11, 14)]}
\newpage
\subsection*{Python Code}
\begin{lstlisting}[language=Python]
def activity_selection(activities):
    # Sort activities by their end time
    activities.sort(key=lambda x: x[1])

    selected = []
    end_time = 0

    for start, finish in activities:
        # If current activity starts after or at the end of last selected
        if start >= end_time:
            selected.append((start, finish))  # Select activity
            end_time = finish                 # Update end time

    return selected
\end{lstlisting}

\subsection*{How to Give Input}

The input should be a list of tuples, where each tuple represents an activity with its start and end times.

\begin{lstlisting}[language=Python]
activities = [(1, 3), (2, 5), (4, 7), (1, 8),
              (5, 9), (8, 10), (9, 11), 
              (11, 14), (13, 16)]
result = activity_selection(activities)
print(result)
\end{lstlisting}


\section*{5. Example 2: Huffman Coding}

\subsection*{Problem}
Given characters and their frequencies, build an optimal prefix code that minimizes total encoding length.

\subsection*{Greedy Strategy}
Merge the two least frequent nodes at each step to build the tree.

\subsection*{Example}

Characters and Frequencies:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Character & Frequency \\
\hline
A & 5 \\
B & 9 \\
C & 12 \\
D & 13 \\
E & 16 \\
F & 45 \\
\hline
\end{tabular}
\end{center}

\begin{itemize}
    \item Step 1: Combine A(5) + B(9) → Node(14)
    \item Step 2: Combine C(12) + D(13) → Node(25)
    \item Step 3: Combine Node(14) + E(16) → Node(30)
    \item Step 4: Combine Node(25) + Node(30) → Node(55)
    \item Step 5: Combine Node(55) + F(45) → Node(100)
\end{itemize}

\textbf{Result:} A binary tree with optimal prefix codes based on traversal.


\newpage
\subsection*{Python Code}
\begin{lstlisting}[language=Python]
import heapq

# Define a tree node
class Node:
    def __init__(self, char, freq):
        self.char = char      # Character (None for internal nodes)
        self.freq = freq      # Frequency of the character
        self.left = None      # Left child
        self.right = None     # Right child

    # Comparison operator for priority queue (min-heap)
    def __lt__(self, other):
        return self.freq < other.freq

def huffman_coding(char_freq):
    # Create a heap with one node per character
    heap = [Node(c, f) for c, f in char_freq.items()]
    heapq.heapify(heap)

    # Build the Huffman Tree
    while len(heap) > 1:
        left = heapq.heappop(heap)   # Least frequent node
        right = heapq.heappop(heap)  # Next least frequent

        # Merge both nodes
        merged = Node(None, left.freq + right.freq)
        merged.left = left
        merged.right = right

        heapq.heappush(heap, merged)  # Insert merged node back

    return heap[0]  # Return the root of the Huffman Tree
\end{lstlisting}

\newpage
\subsection*{How to Give Input}

The input should be a dictionary where the keys are characters and values are their corresponding frequencies.

\begin{lstlisting}[language=Python]
char_freq = {
    'A': 5,
    'B': 9,
    'C': 12,
    'D': 13,
    'E': 16,
    'F': 45
}

root = huffman_coding(char_freq)
# To generate the codes, perform a traversal on the tree.
\end{lstlisting}


\section*{6. Example 3: Fractional Knapsack}

\subsection*{Problem}
Given $n$ items with value and weight, and knapsack capacity $W$, maximize value with possible fractional items.

\subsection*{Greedy Strategy}
Pick items in descending order of value-to-weight ratio.

\subsection*{Example}

Items:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Item & Value & Weight \\
\hline
1 & 60 & 10 \\
2 & 100 & 20 \\
3 & 120 & 30 \\
\hline
\end{tabular}
\end{center}

Knapsack capacity = 50

\newpage

\begin{itemize}
    \item Compute value/weight ratios:  
    Item1 = 6, Item2 = 5, Item3 = 4
    \item Sort by ratio: Item1, Item2, Item3
    \item Take all of Item1 → Remaining = 40
    \item Take all of Item2 → Remaining = 20
    \item Take $20/30$ of Item3 = $120 \times \frac{2}{3} = 80$
\end{itemize}

\textbf{Total value:} $60 + 100 + 80 = 240$

\subsection*{Python Code}
\begin{lstlisting}[language=Python]
def fractional_knapsack(capacity, items):
    # items: list of tuples (value, weight)

    # Sort items by value/weight ratio in descending order
    items.sort(key=lambda x: x[0]/x[1], reverse=True)

    total_value = 0.0  # Store maximum total value

    for value, weight in items:
        if capacity >= weight:
            # Take full item
            total_value += value
            capacity -= weight
        else:
            # Take fractional part
            total_value += value * (capacity / weight)
            break  # Knapsack is full

    return total_value
\end{lstlisting}

\subsection*{How to Give Input}

The input should be a list of tuples where each tuple is (value, weight), and an integer for knapsack capacity.

\begin{lstlisting}[language=Python]
items = [(60, 10), (100, 20), (120, 30)]
capacity = 50

max_value = fractional_knapsack(capacity, items)
print(max_value)
\end{lstlisting}

\section*{7. Example 4: Job Sequencing with Deadline}

\subsection*{Problem Statement}

You are given $n$ jobs. Each job has:
\begin{itemize}
    \item A unique identifier (Job ID)
    \item A deadline by which it must be finished
    \item A profit associated with completing it on or before the deadline
\end{itemize}

Each job takes 1 unit of time. The goal is to schedule jobs to maximize total profit such that no two jobs overlap and each is finished by its deadline.

\subsection*{Greedy Strategy}

\begin{itemize}
    \item Sort all jobs in descending order of profit
    \item Iterate through each job and try to place it in the latest available slot before its deadline
    \item Use a time slot array to track which time units are occupied
\end{itemize}

\begin{tcolorbox}[colback=white, colframe=black, title=Key Idea]
Choose the most profitable jobs first and place them as late as possible (before or on their deadline) to keep earlier slots open for other jobs.
\end{tcolorbox}

\subsection*{Example}

Jobs:  
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Job ID & Deadline & Profit \\
\hline
J1 & 2 & 100 \\
J2 & 1 & 19 \\
J3 & 2 & 27 \\
J4 & 1 & 25 \\
J5 & 3 & 15 \\
\hline
\end{tabular}
\end{center}
\newpage
\begin{itemize}
    \item Sort by profit: J1, J3, J4, J2, J5
    \item Schedule J1 at slot 2
    \item Schedule J3 at slot 1
    \item J4 and J2 can't be scheduled (slots full)
    \item Schedule J5 at slot 3
\end{itemize}

\textbf{Total Profit:} $100 + 27 + 15 = 142$  
\textbf{Selected Jobs:} \texttt{[J3, J1, J5]} in slots \texttt{[1, 2, 3]}

\subsection*{Python Code}
\begin{lstlisting}[language=Python]
# Job class to store each job's properties
class Job:
    def __init__(self, job_id, deadline, profit):
        self.id = job_id
        self.deadline = deadline
        self.profit = profit

def job_sequencing(jobs):
    # Sort jobs by descending profit
    jobs.sort(key=lambda x: x.profit, reverse=True)

    # Find maximum deadline to size the time slot array
    max_deadline = max(job.deadline for job in jobs)
    slots = [False] * (max_deadline + 1)  # 1-based indexing
    result = [None] * (max_deadline + 1)

    total_profit = 0

    for job in jobs:
        # Try to find a free slot from job.deadline down to 1
        for t in range(job.deadline, 0, -1):
            if not slots[t]:
                slots[t] = True
                result[t] = job.id
                total_profit += job.profit
                break

    scheduled_jobs = [job_id for job_id in result if job_id is not None]
    return scheduled_jobs, total_profit
\end{lstlisting}

\subsection*{How to Give Input}

You need to define a list of `Job` objects with their IDs, deadlines, and profits.

\begin{lstlisting}[language=Python]
# Define jobs
jobs = [
    Job("J1", 2, 100),
    Job("J2", 1, 19),
    Job("J3", 2, 27),
    Job("J4", 1, 25),
    Job("J5", 3, 15)
]

# Run algorithm
scheduled, profit = job_sequencing(jobs)

# Output
print("Scheduled Jobs:", scheduled)
print("Total Profit:", profit)
\end{lstlisting}


\section*{8. Example 5: Minimal Cost Spanning Tree (MCST)}

\subsection*{Problem Statement}

Given a connected, undirected, weighted graph, find a spanning tree (subset of edges) that:
\begin{itemize}
    \item Connects all the vertices (no cycles)
    \item Has the minimum possible total edge weight
\end{itemize}

This is known as a \textbf{Minimum Spanning Tree (MST)}.

\newpage
\section*{Prim's Algorithm}

\subsection*{Greedy Strategy}

\begin{itemize}
    \item Start from any node
    \item Always pick the minimum-weight edge that connects a visited vertex to an unvisited vertex
    \item Use a priority queue (min-heap) to efficiently find the next edge
\end{itemize}

\begin{tcolorbox}[colback=white, colframe=black, title=Key Idea]
Build the MST incrementally by always adding the smallest edge connecting the tree to a new vertex.
\end{tcolorbox}

\subsection*{Example}

Graph with 5 vertices and following weighted edges:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
From & To & Weight \\
\hline
0 & 1 & 2 \\
0 & 3 & 6 \\
1 & 2 & 3 \\
1 & 3 & 8 \\
1 & 4 & 5 \\
2 & 4 & 7 \\
3 & 4 & 9 \\
\hline
\end{tabular}
\end{center}

\textbf{Output:} Total cost = 16  
Edges: (0-1), (1-2), (1-4), (0-3)

\newpage

\subsection*{Python Code}
\begin{lstlisting}[language=Python]
import heapq
from collections import defaultdict

def prims_mst(graph, start=0):
    visited = set()
    min_heap = [(0, start)]  # (cost, node)
    total_cost = 0

    while min_heap:
        cost, u = heapq.heappop(min_heap)
        if u in visited:
            continue
        visited.add(u)
        total_cost += cost
        for v, weight in graph[u]:
            if v not in visited:
                heapq.heappush(min_heap, (weight, v))

    return total_cost
\end{lstlisting}

\subsection*{How to Give Input (Prim's)}

\begin{lstlisting}[language=Python]
# Define graph as an adjacency list
graph = {
    0: [(1, 2), (3, 6)],
    1: [(0, 2), (2, 3), (3, 8), (4, 5)],
    2: [(1, 3), (4, 7)],
    3: [(0, 6), (1, 8), (4, 9)],
    4: [(1, 5), (2, 7), (3, 9)]
}

# Run Prim's algorithm
cost = prims_mst(graph, start=0)
print("Total cost of MST:", cost)
\end{lstlisting}

\newpage

\section*{Kruskal's Algorithm}

\subsection*{Greedy Strategy}

\begin{itemize}
    \item Sort all edges by weight
    \item Use Disjoint Set Union (DSU) to check whether adding an edge creates a cycle
    \item Add edge only if it connects two different components
\end{itemize}

\begin{tcolorbox}[colback=white, colframe=black, title=Key Idea]
Always pick the smallest edge that doesn’t form a cycle with the edges already chosen.
\end{tcolorbox}

\subsection*{Example}

Same graph as in Prim’s Example.

\textbf{Sorted Edges:}  
(0-1), (1-2), (1-4), (0-3), (1-3), (2-4), (3-4)

\textbf{Selected Edges:}  
(0-1), (1-2), (1-4), (0-3)

\textbf{Total Cost:} 16

\newpage

\subsection*{Python Code}
\begin{lstlisting}[language=Python]
class DSU:
    def __init__(self, n):
        self.parent = list(range(n))

    def find(self, u):
        if self.parent[u] != u:
            self.parent[u] = self.find(self.parent[u])
        return self.parent[u]

    def union(self, u, v):
        pu, pv = self.find(u), self.find(v)
        if pu == pv:
            return False
        self.parent[pu] = pv
        return True

def kruskal_mst(n, edges):
    edges.sort(key=lambda x: x[2])  # Sort by weight
    dsu = DSU(n)
    total_cost = 0

    for u, v, weight in edges:
        if dsu.union(u, v):
            total_cost += weight

    return total_cost
\end{lstlisting}

\newpage

\subsection*{How to Give Input (Kruskal's)}

\begin{lstlisting}[language=Python]
# Define number of nodes
n = 5

# Define edges as (u, v, weight)
edges = [
    (0, 1, 2),
    (0, 3, 6),
    (1, 2, 3),
    (1, 3, 8),
    (1, 4, 5),
    (2, 4, 7),
    (3, 4, 9)
]

# Run Kruskal's algorithm
cost = kruskal_mst(n, edges)
print("Total cost of MST:", cost)
\end{lstlisting}


\section*{9. Example 6: Dijkstra's Shortest Path Algorithm}

\subsection*{Problem Statement}

Given a graph with non-negative edge weights, find the shortest distance from a source vertex to all other vertices in the graph.

\textbf{Input:}  
A connected, weighted, directed/undirected graph with $n$ vertices and $m$ edges.

\textbf{Output:}  
Shortest distance from the source vertex to all other vertices.

\newpage

\subsection*{Greedy Strategy}

\begin{itemize}
    \item Initialize distances from the source to all vertices as infinity, except the source itself (0).
    \item Use a priority queue (min-heap) to always expand the vertex with the smallest known distance.
    \item Update distances to its neighbors if a shorter path is found via the current node.
    \item Repeat until all vertices are processed.
\end{itemize}

\begin{tcolorbox}[colback=white, colframe=black, title=Key Idea]
Always expand the node with the smallest known distance. Once a node is finalized (visited), its shortest path is guaranteed.
\end{tcolorbox}

\subsection*{Example}

Graph:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
From & To & Weight \\
\hline
0 & 1 & 4 \\
0 & 2 & 1 \\
2 & 1 & 2 \\
1 & 3 & 1 \\
2 & 3 & 5 \\
3 & 4 & 3 \\
\hline
\end{tabular}
\end{center}

\textbf{Source:} 0

\textbf{Distances from 0:}  
0 → 0,  
1 → 3 (via 2),  
2 → 1,  
3 → 4,  
4 → 7

\newpage

\subsection*{Python Code}
\begin{lstlisting}[language=Python]
import heapq
from collections import defaultdict

def dijkstra(graph, source):
    # Initialize distance to all nodes as infinity
    dist = {node: float('inf') for node in graph}
    dist[source] = 0

    # Min-heap to get the node with the smallest distance
    heap = [(0, source)]

    while heap:
        current_dist, u = heapq.heappop(heap)

        # Skip if we already found a better path
        if current_dist > dist[u]:
            continue

        for v, weight in graph[u]:
            if dist[u] + weight < dist[v]:
                dist[v] = dist[u] + weight
                heapq.heappush(heap, (dist[v], v))

    return dist
\end{lstlisting}

\subsection*{How to Give Input}

\begin{lstlisting}[language=Python]
# Define graph as adjacency list
graph = {
    0: [(1, 4), (2, 1)],
    1: [(3, 1)],
    2: [(1, 2), (3, 5)],
    3: [(4, 3)],
    4: []
}

source = 0
shortest_paths = dijkstra(graph, source)

# Output the result
for node in sorted(shortest_paths):
    print(f"Distance from {source} to {node} is {shortest_paths[node]}")
\end{lstlisting}

\subsection*{Python Code (Adjacency Matrix)}
\begin{lstlisting}[language=Python]
import heapq

def dijkstra_matrix(adj_matrix, source):
    n = len(adj_matrix)
    dist = [float('inf')] * n
    visited = [False] * n
    dist[source] = 0

    min_heap = [(0, source)]  # (distance, node)

    while min_heap:
        d, u = heapq.heappop(min_heap)

        if visited[u]:
            continue
        visited[u] = True

        for v in range(n):
            weight = adj_matrix[u][v]
            if weight != 0 and not visited[v]:
                if dist[u] + weight < dist[v]:
                    dist[v] = dist[u] + weight
                    heapq.heappush(min_heap, (dist[v], v))

    return dist
\end{lstlisting}

\subsection*{How to Give Input (Adjacency Matrix)}
\begin{lstlisting}[language=Python]
# 0 means no direct edge between the nodes
adj_matrix = [
    [0, 4, 1, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 2, 0, 5, 0],
    [0, 0, 0, 0, 3],
    [0, 0, 0, 0, 0]
]

source = 0
distances = dijkstra_matrix(adj_matrix, source)
# Output the result
for i, d in enumerate(distances):
    print(f"Distance from {source} to {i} is {d}")
\end{lstlisting}

\subsection*{Comparison: Adjacency List vs Adjacency Matrix}

\begin{center}
\begin{tabular}{|l|p{6cm}|p{6cm}|}
\hline
\textbf{Feature} & \textbf{Adjacency List} & \textbf{Adjacency Matrix} \\
\hline
Memory Efficiency & Efficient for sparse graphs & Wastes space in sparse graphs due to many 0s \\
\hline
Edge Lookup Time & Slower — requires looping through neighbors & Faster — direct access in $O(1)$ time \\
\hline
Implementation & Easier for dynamic graphs & Easier for fixed-size graphs \\
\hline
Best Use Case & Sparse graphs (fewer edges) & Dense graphs (many edges) \\
\hline
Time Complexity & $O((V + E)\log V)$ using heap & $O(V^2)$ in worst case \\
\hline
\end{tabular}
\end{center}


\subsection*{10. Understanding Edge Relaxation in Dijkstra's Algorithm}

\textbf{Relaxation} is the process of updating the shortest known distance to a vertex if a shorter path is found.

\begin{tcolorbox}[colback=white, colframe=black, title=What is Edge Relaxation?]
For an edge $(u, v)$ with weight $w$, relaxation checks if going from source $\rightarrow u \rightarrow v$ is shorter than the currently known distance to $v$:

\[
\text{if } \texttt{dist[u] + w < dist[v]} \text{ then update } \texttt{dist[v] = dist[u] + w}
\]

This ensures that we always maintain the shortest known distance to each node.
\end{tcolorbox}

\vspace{0.5cm}

\subsubsection*{Relaxation Line in Adjacency List Code}
\begin{lstlisting}[language=Python]
if dist[u] + weight < dist[v]:
    dist[v] = dist[u] + weight      # <-- Relaxation happens here
    heapq.heappush(heap, (dist[v], v))
\end{lstlisting}

\vspace{1cm}

\subsubsection*{Relaxation Line in Adjacency Matrix Code}
\begin{lstlisting}[language=Python]
if dist[u] + weight < dist[v]:
    dist[v] = dist[u] + weight      # <-- Relaxation happens here
    heapq.heappush(min_heap, (dist[v], v))
\end{lstlisting}

\vspace{0.5cm}

\begin{tcolorbox}[colback=white, colframe=black, title=Note]
Relaxation is the heart of Dijkstra’s algorithm. It is applied every time a shorter path to a neighbor is found via the current node.
\end{tcolorbox}


\section*{11. Time \& Space Complexity Summary}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Problem}                & \textbf{Time}               & \textbf{Space}         & \textbf{Optimal} \\
\hline
Activity Selection              & $O(n \log n)$               & $O(1)$                 & Yes              \\
Huffman Coding                  & $O(n \log n)$               & $O(n)$                 & Yes              \\
Fractional Knapsack             & $O(n \log n)$               & $O(1)$                 & Yes              \\
Job Sequencing with Deadline    & $O(n \log n)$               & $O(n)$                 & Yes              \\
Prim's Algorithm (List)         & $O((V + E) \log V)$         & $O(V + E)$             & Yes              \\
Prim's Algorithm (Matrix)       & $O(V^2)$                    & $O(V^2)$               & Yes              \\
Kruskal's Algorithm             & $O(E \log E)$               & $O(V)$ (DSU)           & Yes              \\
Dijkstra (Adjacency List)       & $O((V + E) \log V)$         & $O(V + E)$             & Yes              \\
Dijkstra (Adjacency Matrix)     & $O(V^2)$                    & $O(V^2)$               & Yes              \\
\hline
\end{tabular}
\end{center}

\newpage

\section*{12. Points to Remember}

\begin{itemize}
    \item Greedy algorithms make a locally optimal choice at each step with the hope of reaching a global optimum.
    \item They do not always work — correctness depends on satisfying the \textbf{Greedy Choice Property} and \textbf{Optimal Substructure}.
    \item Sorting is often the first step (e.g., by profit, deadline, frequency, ratio, etc.).
    \item Greedy algorithms are usually more efficient than dynamic programming.
    \item Many problems (e.g., Fractional Knapsack) can be solved using greedy methods, but not all (e.g., 0/1 Knapsack).
    \item Activity Selection and Job Sequencing both rely on choosing based on deadline or end time.
    \item Huffman Coding builds a binary tree using a priority queue (min-heap).
    \item Fractional Knapsack allows taking parts of an item — hence greedy works perfectly.
    \item In Job Sequencing, use a time slot array to track available deadlines.
    \item Prim’s Algorithm grows a tree from any starting vertex by choosing the lightest connecting edge.
    \item Kruskal’s Algorithm sorts all edges and builds the MST by adding edges that don’t form a cycle.
    \item Dijkstra’s Algorithm uses a min-heap to expand the shortest known node first — it only works for non-negative weights.
    \item Greedy algorithms typically have lower space complexity ($O(1)$ to $O(n)$).
    \item Always prove or validate the correctness of a greedy algorithm — don’t assume it will always work.
    \item Common applications include scheduling, compression, spanning trees, and pathfinding.
\end{itemize}

\section*{ Final Summary}

\begin{tcolorbox}[colback=white, colframe=black, title=What You Should Know About Greedy Algorithms]
\begin{itemize}
    \item Greedy algorithms make \textbf{locally optimal} choices at each step, aiming for a \textbf{globally optimal} solution.
    \item They are efficient both in time and space, often outperforming dynamic programming in simple problems.
    \item Correctness depends on:
    \begin{itemize}
        \item \textbf{Greedy Choice Property}
        \item \textbf{Optimal Substructure}
    \end{itemize}
    \item Not all problems are solvable using greedy — always analyze if the greedy conditions are met.
\end{itemize}
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[colback=white, colframe=black, title=Key Problems Solved Using Greedy]
\begin{multicols}{2}
\begin{itemize}[leftmargin=*, noitemsep]
    \item Activity Selection
    \item Fractional Knapsack
    \item Huffman Coding
    \item Job Sequencing with Deadline
    \item Prim's Algorithm (MST)
    \item Kruskal's Algorithm (MST)
    \item Dijkstra's Algorithm (Shortest Path)
\end{itemize}
\end{multicols}
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[colback=white, colframe=black, title=Tips]
\begin{itemize}
    \item Sort data first based on the problem requirement (e.g., profit, deadline, ratio).
    \item Use data structures like heaps or DSU to improve performance.
    \item Greedy is ideal for real-time and approximation problems.
    \item Validate greedy correctness via proof or counter-example testing.
\end{itemize}
\end{tcolorbox}


\end{document}
